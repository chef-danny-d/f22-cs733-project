{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d8ff19",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398a6955",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m \u001B[38;5;66;03m# advanced visualization\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split \u001B[38;5;66;03m# for data splitting\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m \u001B[38;5;66;03m# importing tensorflow library\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RobertaTokenizerFast, TFRobertaForSequenceClassification \u001B[38;5;66;03m# for implementing ROBERTa\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AlbertTokenizerFast, TFAlbertForSequenceClassification \u001B[38;5;66;03m# for implementing AlBert\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # to hide irrelevant environment warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to hide irrelevant python warnings\n",
    "\n",
    "from transformers import logging as hf_logging \n",
    "hf_logging.set_verbosity_error() # to hide irrelevant transformers warnings\n",
    "\n",
    "import pandas as pd # for data loading and data analysis\n",
    "import numpy as np # for matrix and linear algebra operations\n",
    "import re # regular expression\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import seaborn as sns # advanced visualization\n",
    "from sklearn.model_selection import train_test_split # for data splitting\n",
    "import tensorflow as tf # importing tensorflow library\n",
    "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification # for implementing ROBERTa\n",
    "from transformers import AlbertTokenizerFast, TFAlbertForSequenceClassification # for implementing AlBert\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification # for implementing DistilBert\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report # evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score # evaluation metrics\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93733d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5457803",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading training set\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head() # showing first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09f2a3",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info() # overview of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf329bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading test set\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.head() # showing first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2c980",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df.info() # overview of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3894e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3b091",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Label Arrangement:\n",
    "\n",
    "0 -> not a disaster\n",
    "\n",
    "1 -> disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020db5c",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_counts = train_df['target'].value_counts() # counts of unique values in target column\n",
    "plt.figure(figsize=(12, 6)) # width and height of graph\n",
    "plt.bar(label_counts.index, label_counts,color ='maroon', width = 0.1) # drawing bar plot\n",
    "plt.title('Label Proportion In Training Set') # title of graph\n",
    "plt.xlabel(\"Label\") # label of x-axis\n",
    "plt.ylabel(\"Count\") # label of y-axis\n",
    "plt.grid(True) # adding grids to graph\n",
    "plt.xticks([0, 1], ['not a disaster', 'disaster']) # putting specific tickers at x-axis\n",
    "plt.show() # showing graph to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac24ed1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41663477",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting text and target information from dataframe\n",
    "train_df = train_df.sample(frac=1)\n",
    "temp = [(x,y) for x,y in zip(list(train_df['text']), list(train_df['target']))]\n",
    "tweets = [t[0] for t in temp] # getting tweets \n",
    "y = [t[1] for t in temp] # getting labels of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daaeb12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Casting target variable to float\n",
    "y = np.array(y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3b3d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    \"\"\"Function Applies preprocessing to tweets.\n",
    "    \n",
    "    Preprocessing appied to tweets include http links removed and html special characters converted\n",
    "    to their respective real characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    tweets : list\n",
    "          list containing sequences of text in the form of human tweets\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    new_tweets : list\n",
    "              list containing preprocessed sequences of text\n",
    "    \"\"\"\n",
    "    new_tweets = [re.sub(r'https?://t.co/\\w+','',t) for t in tweets] # removing http links from text\n",
    "    new_tweets = [re.sub('&lt;','lt',t) for t in new_tweets] # converting html &lt to lt\n",
    "    new_tweets = [re.sub('&gt;','gt',t) for t in new_tweets] # converting html &gt to gt\n",
    "    new_tweets = [re.sub('&amp;','&',t) for t in new_tweets] # converting html &amp to &\n",
    "    return new_tweets # returning preprocessed tweets\n",
    "\n",
    "tweets = preprocess_tweets(tweets) # applying preprocessing to tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1724dcd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ROBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9addd",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'roberta-base' # official name of roberta model\n",
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(model_name) # applying tokenization to roberta\n",
    "roberta_seq = TFRobertaForSequenceClassification.from_pretrained(model_name) # implementing roberta on sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1619f",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "roberta_seq.summary() # summary of the initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb654ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Combining all data in a separate list just for determining the length of sequences\n",
    "all_tweets = list(pd.concat([train_df, test_df], axis=0)['text'])\n",
    "all_tweets = preprocess_tweets(all_tweets)\n",
    "max_len = max([len(t) for t in roberta_tokenizer(all_tweets)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94317281",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# performing data splitting based on ratio 80/20 -> 80% for training and 20% for validation\n",
    "length = len(tweets)\n",
    "X_train = tweets[:int(length*0.8)]\n",
    "X_val = tweets[int(length*0.8):]\n",
    "y_train = y[:int(length*0.8)]\n",
    "y_val = y[int(length*0.8):]\n",
    "real_tweets = list(X_val)\n",
    "X_train = roberta_tokenizer(X_train, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "X_val = roberta_tokenizer(X_val, padding='max_length', max_length=max_len, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90062d9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8 # no. of observations passed into single iteration during each epoch\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea66568",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6) # applying Adam optimizer to neural network\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # applying loss function\n",
    "roberta_seq.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) # compiling roberta model with loss, optimizer and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c15792",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = './roberta_checkpoint' # directory name where model weights will be stored\n",
    "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, \n",
    "                                                         monitor='val_accuracy',\n",
    "                                                         mode='max',\n",
    "                                                         save_weights_only=True, \n",
    "                                                         save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b91ed8",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "roberta_history = roberta_seq.fit(train_dataset, epochs=3, validation_data=val_dataset, callbacks=[callback_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d8191",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "roberta_seq.load_weights(checkpoint) # loading weights from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ffc43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputs = roberta_seq.predict(val_dataset, verbose=False) # making predictions on validation set\n",
    "roberta_y_pred = outputs[0].argmax(axis=1)\n",
    "roberta_predictions = ['No' if val == 0 else 'Yes' for val in roberta_y_pred.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3297fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "roberta_accuracy = round(accuracy_score(y_val, roberta_y_pred), 2)\n",
    "print('Accuracy: {}'.format(roberta_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef35cd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_val, roberta_y_pred, labels=[0, 1], target_names=['not a disaster','disaster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c71968",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "roberta_precision = round(precision_score(y_val, roberta_y_pred), 2)\n",
    "roberta_recall = round(recall_score(y_val, roberta_y_pred), 2)\n",
    "roberta_f1 = round(f1_score(y_val, roberta_y_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df9550",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualization of confusion matrix\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.figure(figsize=(16, 8))\n",
    "cm = confusion_matrix(y_val, roberta_y_pred)\n",
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=np.unique(y_val), yticklabels=np.unique(roberta_y_pred))\n",
    "plt.title('\\nConfusion Matrix')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9773a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# applying tokenization on test set\n",
    "tweets_test = list(test_df['text'])\n",
    "tweets_test = preprocess_tweets(tweets_test)\n",
    "X_test = roberta_tokenizer(tweets_test, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(dict(X_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2ea33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# making predictions on test data\n",
    "outputs_test = roberta_seq.predict(test_dataset, verbose=False)\n",
    "y_pred_test = outputs_test[0].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8452cd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# saving results to file\n",
    "results = pd.DataFrame()\n",
    "results['id'] = test_df['id']\n",
    "results['target'] = y_pred_test\n",
    "results.to_csv('./roberta_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9716184",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# AlBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772138ae",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'albert-base-v2'\n",
    "albert_tokenizer = AlbertTokenizerFast.from_pretrained(model_name)\n",
    "albert_seq = TFAlbertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba98dd6",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "albert_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52146d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Combining all data in a separate list just for determining the length of sequences\n",
    "all_tweets = list(pd.concat([train_df, test_df],axis=0)['text'])\n",
    "all_tweets = preprocess_tweets(all_tweets)\n",
    "max_len = max([len(t) for t in albert_tokenizer(all_tweets)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4097d9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "length = len(tweets)\n",
    "X_train = tweets[:int(length*0.8)]\n",
    "X_val = tweets[int(length*0.8):]\n",
    "y_train = y[:int(length*0.8)]\n",
    "y_val = y[int(length*0.8):]\n",
    "X_train = albert_tokenizer(X_train, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "X_val = albert_tokenizer(X_val, padding='max_length', max_length=max_len, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95382ca3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba793eb2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "albert_seq.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c83b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = './albert_checkpoint'\n",
    "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint,\n",
    "                                              monitor='val_accuracy',\n",
    "                                              mode='max',\n",
    "                                              save_weights_only=True,\n",
    "                                              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac6da9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "albert_history = albert_seq.fit(train_dataset, validation_data=val_dataset, epochs=3, callbacks=[callback_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98105b25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "albert_seq.load_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6137e",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputs = albert_seq.predict(val_dataset, verbose=False)\n",
    "albert_y_pred = outputs[0].argmax(axis=1)\n",
    "albert_predictions = ['No' if val == 0 else 'Yes' for val in albert_y_pred.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac32d93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "albert_accuracy = round(accuracy_score(y_val, albert_y_pred), 2)\n",
    "print('Accuracy: {}'.format(albert_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccfb15",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_val, albert_y_pred, labels=[0, 1], target_names=['not a disaster','disaster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6808fc8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "albert_precision = round(precision_score(y_val, albert_y_pred), 2)\n",
    "albert_recall = round(recall_score(y_val, albert_y_pred), 2)\n",
    "albert_f1 = round(f1_score(y_val, albert_y_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fbbcec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualization of confusion matrix\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.figure(figsize=(16, 8))\n",
    "cm = confusion_matrix(y_val, albert_y_pred)\n",
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=np.unique(y_val), yticklabels=np.unique(albert_y_pred))\n",
    "plt.title('\\nConfusion Matrix')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00ae7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# applying tokenization on test set\n",
    "tweets_test = list(test_df['text'])\n",
    "tweets_test = preprocess_tweets(tweets_test)\n",
    "X_test = albert_tokenizer(tweets_test, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(dict(X_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb785f4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# making predictions on test set\n",
    "outputs_test = albert_seq.predict(test_dataset, verbose=False)\n",
    "y_pred_test = outputs_test[0].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee5342",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# saving results to file\n",
    "results = pd.DataFrame()\n",
    "results['id'] = test_df['id']\n",
    "results['target'] = y_pred_test\n",
    "results.to_csv('./albert_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85371907",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5f3bb",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "distilbert_seq = TFDistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb0f58",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "distilbert_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee1737",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Combining all data in a separate list just for determining the length of sequences\n",
    "all_tweets = list(pd.concat([train_df, test_df], axis=0)['text'])\n",
    "all_tweets = preprocess_tweets(all_tweets)\n",
    "max_len = max([len(t) for t in distilbert_tokenizer(all_tweets)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321894f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "length = len(tweets)\n",
    "X_train = tweets[:int(length*0.8)]\n",
    "X_val = tweets[int(length*0.8):]\n",
    "y_train = y[:int(length*0.8)]\n",
    "y_val = y[int(length*0.8):]\n",
    "X_train = distilbert_tokenizer(X_train, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "X_val = distilbert_tokenizer(X_val, padding='max_length', max_length=max_len, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e72d0f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5440004",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "distilbert_seq.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74e6a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = './distilbert_checkpoint'\n",
    "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint, \n",
    "                                                    monitor='val_accuracy',\n",
    "                                                    mode='max',\n",
    "                                                    save_weights_only=True,\n",
    "                                                    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725487c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "distilbert_history = distilbert_seq.fit(train_dataset, validation_data=val_dataset, epochs=3, callbacks=[callback_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be349f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "distilbert_seq.load_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebcceb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputs = distilbert_seq.predict(val_dataset, verbose=False)\n",
    "distilbert_y_pred = outputs[0].argmax(axis=1)\n",
    "distilbert_predictions = ['No' if val == 0 else 'Yes' for val in distilbert_y_pred.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966d1a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "distilbert_accuracy = round(accuracy_score(y_val, distilbert_y_pred), 2)\n",
    "print('Accuracy: {}'.format(distilbert_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d6ffe",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_val, distilbert_y_pred, labels=[0, 1], target_names=['not a disaster','disaster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a9cb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "distilbert_precision = round(precision_score(y_val, distilbert_y_pred), 2)\n",
    "distilbert_recall = round(recall_score(y_val, distilbert_y_pred), 2)\n",
    "distilbert_f1 = round(f1_score(y_val, distilbert_y_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c4774",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualization of confusion matrix\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.figure(figsize=(16, 8))\n",
    "cm = confusion_matrix(y_val, distilbert_y_pred)\n",
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=np.unique(y_val), yticklabels=np.unique(distilbert_y_pred))\n",
    "plt.title('\\nConfusion Matrix')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bfb67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# applying tokenization on test set\n",
    "tweets_test = list(test_df['text'])\n",
    "tweets_test = preprocess_tweets(tweets_test)\n",
    "X_test = distilbert_tokenizer(tweets_test, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(dict(X_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec1cde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# making predictions on test set\n",
    "outputs_test = distilbert_seq.predict(test_dataset, verbose=False)\n",
    "y_pred_test = outputs_test[0].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c94c53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# saving results to file\n",
    "results = pd.DataFrame()\n",
    "results['id'] = test_df['id']\n",
    "results['target'] = y_pred_test\n",
    "results.to_csv('./distilbert_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f4314",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "roberta_acc_history = roberta_history.history['val_accuracy']\n",
    "albert_acc_history = albert_history.history['val_accuracy']\n",
    "distilbert_acc_history = distilbert_history.history['val_accuracy']\n",
    "epochs = range(1, 4)\n",
    "plt.plot(epochs, roberta_acc_history, label='ROBERTa')\n",
    "plt.plot(epochs, albert_acc_history, label='AlBert')\n",
    "plt.plot(epochs, distilbert_acc_history, label='DistilBert')\n",
    "plt.title('Validation Accuracies Of ROBERTa, AlBert, and DistilBert During Each Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3926fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Comparison of Results Of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecb5a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "columns = ['ROBERTa', 'AlBert', 'DistilBert']\n",
    "roberta = [roberta_accuracy, roberta_precision, roberta_recall, roberta_f1]\n",
    "albert = [albert_accuracy, albert_precision, albert_recall, albert_f1]\n",
    "distilbert = [distilbert_accuracy, distilbert_precision, distilbert_recall, distilbert_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785755e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dict_ = {'ROBERTa':roberta,\n",
    "         'AlBert':albert,\n",
    "         'DistilBert':distilbert}\n",
    "results_df = pd.DataFrame(dict_, index=index).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c247fd5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ec485",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# showing real tweets, true lables and predictions by three models\n",
    "new_y_val = ['No' if val == 0 else 'Yes' for val in y_val.tolist()]\n",
    "labels_dict = {'Tweet':real_tweets,\n",
    "             'Real Label':new_y_val,\n",
    "             'ROBERTa':roberta_predictions,\n",
    "             'AlBert':albert_predictions,\n",
    "             'DistilBert':distilbert_predictions}\n",
    "labels_df = pd.DataFrame(labels_dict)\n",
    "labels_df = labels_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e65556",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}